{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "from typing import Sequence, Any, Iterable, Optional, List\n",
    "import numpy as np\n",
    "# import click\n",
    "# import click_log\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as tnnf\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTLayer(nn.Module):\n",
    "    def __init__(self, in_factors, out_factors, ranks, ein_string):\n",
    "        super().__init__()\n",
    "        self.in_factors = in_factors\n",
    "        self.out_factors = out_factors\n",
    "        self.ein_string = ein_string\n",
    "        assert len(in_factors) == len(out_factors) == len(ranks) + 1, 'Input factorization should match output factorization and should be equal to len(ranks) - 1'\n",
    "#         assert len(ranks) == 4, 'Now we consider particular factorization for given dataset'\n",
    "\n",
    "        self.cores = [nn.Parameter(torch.randn(in_factors[0], 1, ranks[0], out_factors[0], ) * 0.8)]\n",
    "        for i in range(1, len(in_factors) - 1):\n",
    "            self.cores.append(nn.Parameter(torch.randn(in_factors[0], ranks[i-1], ranks[i], out_factors[0],) * 0.1))\n",
    "        self.cores.append(nn.Parameter(torch.randn(in_factors[-1], ranks[-1], 1, out_factors[-1], ) * 0.8))\n",
    "#         print(self.cores)\n",
    "    def forward(self, x):\n",
    "        reshaped_input = x.reshape(-1, *self.in_factors)\n",
    "#         print('reshaped_input', reshaped_input.shape)\n",
    "        # in the einsum below, n stands for index of sample in the batch,\n",
    "        # abcde - indices corresponding to h1, h2, hw, w1, w2 modes\n",
    "        # o, i, j, k, l, p - indices corresponding to the 4 tensor train ranks\n",
    "        # v, w, x, y, z - indices corresponding to o1, o2, o3, o4, o5\n",
    "\n",
    "        result = torch.einsum(\n",
    "            self.ein_string,\n",
    "            reshaped_input, *self.cores\n",
    "        )\n",
    "        return result.reshape(-1, np.prod(self.out_factors))\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.cores\n",
    "\n",
    "class TTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Upsample(size=cfg.resize_shape, mode=\"bilinear\", align_corners=False),\n",
    "            TTLayer(cfg.in_factors, cfg.hidd_out_factors, cfg.l1_ranks, cfg.ein_string1),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(np.prod(hidd_factors), NUM_LABELS),\n",
    "            TTLayer(cfg.hidd_in_factors, cfg.out_factors, cfg.l2_ranks, cfg.ein_string2),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "        # self.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    def parameters(self,):\n",
    "        return self.net[1].parameters() + list(self.net[3].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'resize_shape': (32, 32),\n",
    "    \n",
    "    'in_factors': (4, 4, 4, 4, 4),\n",
    "    'l1_ranks': (8, 8, 8, 8),\n",
    "    'hidd_out_factors': (2, 2, 2, 2, 2),\n",
    "    'ein_string1': \"nabcde,aoiv,bijw,cjkx,dkly,elpz\",\n",
    "    \n",
    "    'hidd_in_factors': (4, 8),\n",
    "    'l2_ranks': (16,),\n",
    "    'out_factors': (5, 2),\n",
    "    'ein_string2': 'nab,aoix,bipy',\n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "cfg = AttrDict(config)\n",
    "model = TTModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from ignite.metrics import Loss, Accuracy\n",
    "# from ignite.engine import Events, create_supervised_evaluator\n",
    "# from ignite.contrib.handlers.param_scheduler import LRScheduler\n",
    "\n",
    "# from libcrap import shuffled\n",
    "# from libcrap.torch import set_random_seeds\n",
    "# from libcrap.torch.click import (\n",
    "#     click_dataset_root_option, click_models_dir_option, click_tensorboard_log_dir_option,\n",
    "#     click_seed_and_device_options\n",
    "# )\n",
    "# from libcrap.torch.training import (\n",
    "#     add_checkpointing, add_early_stopping, add_weights_and_grads_logging,\n",
    "#     setup_trainer, setup_evaluator, setup_tensorboard_logger,\n",
    "#     make_standard_prepare_batch_with_events, add_logging_input_images\n",
    "# )\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "MNIST_DATASET_SIZE = 60000\n",
    "NUM_LABELS = 10\n",
    "\n",
    "MNIST_TRANSFORM = transforms.Compose((\n",
    "    transforms.Pad(2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1,), (0.2752,))\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "train_dataset_size = 40000\n",
    "batch_size = 10\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 10\n",
    "dataset = MNIST('mnist', train=True, download=True, transform=MNIST_TRANSFORM)\n",
    "assert len(dataset) == MNIST_DATASET_SIZE\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, (train_dataset_size, MNIST_DATASET_SIZE - train_dataset_size)\n",
    ")\n",
    "\n",
    "train_loader, val_loader = (\n",
    "    DataLoader(\n",
    "        dataset_, batch_size=batch_size, shuffle=True, pin_memory=(device.type == \"cuda\")\n",
    "    )\n",
    "    for dataset_ in (train_dataset, val_dataset)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=learning_rate, momentum=0.95, weight_decay=0.0005\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a73d3b9e31f450cb69561972d205629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f7b6686a6a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         print(loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/envs-wHylL7Rh/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/envs-wHylL7Rh/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lf = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for ep in range(n_epochs):\n",
    "    for b, gt in tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "#         b = b.cuda()\n",
    "        out = model(b)\n",
    "#         print(out.argmax(1), gt)\n",
    "        loss = lf(out, gt.to(device))\n",
    "#         print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(acc(model, val_loader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(model, loader):\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for b, gt in tqdm(loader):\n",
    "            out = model(b).argmax(1).numpy()\n",
    "            gt = gt.numpy()\n",
    "            accs.append(sum(out == gt) / len(out))\n",
    "    return sum(accs) / len(accs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f3c977e2c4440a886afdd976e71811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6506500000000003"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
